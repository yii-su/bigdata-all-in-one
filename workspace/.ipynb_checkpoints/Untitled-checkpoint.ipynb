{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9706ee8b-05a4-4adf-8d59-cb6de688c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建SparkSession并启用Hive支持\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkHiveExample\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:8020/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#     CREATE DATABASE tdb1;\n",
    "# \"\"\").show()\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#     CREATE TABLE tdb1.test_table (id INT, name STRING);\n",
    "# \"\"\")\n",
    "#     # INSERT INTO test_table VALUES (1, 'Alice'), (2, 'Bob');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41cff34-5065-44a3-95d3-13680ec48164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "#     INSERT INTO tdb1.test_table VALUES (1, 'Alice'), (2, 'Bob');\n",
    "# \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2737e022-fe4f-401b-ba1e-a4a64e17ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from tdb1.test_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96cfdab-7ba5-4c62-bc24-13b11ead4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93ccb85-01f1-4907-9960-52edc7986ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|Hello, HDFS!|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadHDFS\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:8020\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 读取HDFS上的文件\n",
    "# 假设HDFS上的文件路径为 hdfs://namenode:8020/user/hadoop/input.txt\n",
    "hdfs_file_path = \"hdfs://namenode:8020/user/hadoop/a.txt\"\n",
    "\n",
    "# 读取文本文件\n",
    "df = spark.read.text(hdfs_file_path)\n",
    "\n",
    "# 显示数据\n",
    "df.show()\n",
    "\n",
    "# 停止SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e78611-4e30-40a3-bfc9-c94df06b7950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
